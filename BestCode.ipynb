{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8p/b3ws2sh942n4mms732sp99fr0000gn/T/ipykernel_28322/4211528759.py:31: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  all_data['day'] = pd.to_datetime(all_data['day'])\n",
      "/var/folders/8p/b3ws2sh942n4mms732sp99fr0000gn/T/ipykernel_28322/4211528759.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  all_data['day'] = pd.to_datetime(all_data['day'], errors='coerce')\n",
      "/Users/macbookpro/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['day_of_week']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 48433, number of negative: 48433\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16946\n",
      "[LightGBM] [Info] Number of data points in the train set: 96866, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "# Banking Dataset Classification\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd  # Importing pandas library for data manipulation and analysis\n",
    "import numpy as np  # Importing numpy library for numerical operations\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV  # Importing functions for data splitting and hyperparameter tuning\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder  # Importing preprocessing tools for scaling and one-hot encoding\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  # Importing ensemble classifiers\n",
    "from lightgbm import LGBMClassifier  # Importing LightGBM classifier\n",
    "from sklearn.impute import SimpleImputer  # Importing SimpleImputer for handling missing data\n",
    "from sklearn.compose import ColumnTransformer  # Importing ColumnTransformer for column-wise transformations\n",
    "from sklearn.pipeline import Pipeline  # Importing Pipeline for creating a processing pipeline\n",
    "from sklearn.metrics import accuracy_score  # Importing accuracy_score for model evaluation\n",
    "from imblearn.over_sampling import BorderlineSMOTE  # Importing BorderlineSMOTE for handling class imbalance\n",
    "import os  # Importing operating system utilities for file handling\n",
    "\n",
    "# Load the training and test data\n",
    "train_data = pd.read_csv('Train-set.csv')  # Load the training data from a CSV file\n",
    "test_data = pd.read_csv('Test-set.csv')    # Load the test data from a CSV file\n",
    "\n",
    "# Separate the 'Target' column from the train data\n",
    "y_train = train_data['Target']  # Storing the target labels in 'y_train'\n",
    "train_data.drop('Target', axis=1, inplace=True)  # Removing the target column from the training data\n",
    "\n",
    "# Combine train and test data for preprocessing\n",
    "all_data = pd.concat([train_data, test_data], axis=0)  # Combining train and test data for combined preprocessing\n",
    "\n",
    "# Feature Engineering: Extract day of the week and create a weekend indicator\n",
    "try:\n",
    "    # Convert 'day' column to datetime\n",
    "    all_data['day'] = pd.to_datetime(all_data['day'])\n",
    "    # Extract day of the week (0-6) and create 'day_of_week' feature\n",
    "    all_data['day_of_week'] = all_data['day'].dt.dayofweek\n",
    "    # Create binary indicator for the weekend (Saturday and Sunday)\n",
    "    all_data['is_weekend'] = all_data['day_of_week'].isin([5, 6]).astype(int)\n",
    "    # Drop the original 'day' column\n",
    "    all_data.drop('day', axis=1, inplace=True)\n",
    "except (ValueError, OverflowError, pd._libs.tslibs.np_datetime.OutOfBoundsDatetime):\n",
    "    # Handle errors due to invalid date formats\n",
    "    all_data['day'] = pd.to_datetime(all_data['day'], errors='coerce')\n",
    "    all_data['day_of_week'] = all_data['day'].dt.dayofweek\n",
    "    all_data['is_weekend'] = all_data['day_of_week'].isin([5, 6]).astype(int)\n",
    "    all_data.drop('day', axis=1, inplace=True)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = all_data.select_dtypes(include=[np.number]).columns  # Identifying numeric columns\n",
    "categorical_cols = all_data.select_dtypes(include=[object]).columns  # Identifying categorical columns\n",
    "\n",
    "# Create transformers for numeric and categorical columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Impute missing values with median\n",
    "    ('scaler', RobustScaler())  # Scale features using robust scaling\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))  # Encode categorical variables using one-hot encoding\n",
    "])\n",
    "\n",
    "# Preprocess the data using the column transformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_cols),  # Apply numeric transformer to numeric columns\n",
    "    ('cat', categorical_transformer, categorical_cols)  # Apply categorical transformer to categorical columns\n",
    "])\n",
    "\n",
    "X_all_preprocessed = preprocessor.fit_transform(all_data)  # Apply preprocessing to all data\n",
    "\n",
    "# Handle class imbalance using BorderlineSMOTE\n",
    "smote = BorderlineSMOTE(sampling_strategy='auto', random_state=42)  # Initialize BorderlineSMOTE for oversampling\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_all_preprocessed[:train_data.shape[0]], y_train)\n",
    "# Apply BorderlineSMOTE to balance classes in the training data\n",
    "\n",
    "# Create and train optimized models\n",
    "optimized_rf_model = RandomForestClassifier(n_estimators=150, max_depth=9, random_state=42)\n",
    "# Initialize RandomForestClassifier with optimized hyperparameters\n",
    "optimized_gb_model = GradientBoostingClassifier(n_estimators=160, learning_rate=0.05, max_depth=7, random_state=42)\n",
    "# Initialize GradientBoostingClassifier with optimized hyperparameters\n",
    "optimized_lgbm_model = LGBMClassifier(n_estimators=180, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "# Initialize LGBMClassifier with optimized hyperparameters\n",
    "\n",
    "optimized_rf_model.fit(X_train_resampled, y_train_resampled)  # Train RandomForestClassifier on resampled data\n",
    "optimized_gb_model.fit(X_train_resampled, y_train_resampled)  # Train GradientBoostingClassifier on resampled data\n",
    "optimized_lgbm_model.fit(X_train_resampled, y_train_resampled)  # Train LGBMClassifier on resampled data\n",
    "\n",
    "# Get predictions using optimized models\n",
    "test_predictions_rf = optimized_rf_model.predict_proba(X_all_preprocessed[train_data.shape[0]:])[:, 1]\n",
    "# Predict probabilities for class 1 using RandomForestClassifier\n",
    "test_predictions_gb = optimized_gb_model.predict_proba(X_all_preprocessed[train_data.shape[0]:])[:, 1]\n",
    "# Predict probabilities for class 1 using GradientBoostingClassifier\n",
    "test_predictions_lgbm = optimized_lgbm_model.predict_proba(X_all_preprocessed[train_data.shape[0]:])[:, 1]\n",
    "# Predict probabilities for class 1 using LGBMClassifier\n",
    "\n",
    "# Combine the predictions using weighted averaging\n",
    "ensemble_predictions = (0.4 * test_predictions_rf) + (0.4 * test_predictions_gb) + (0.2 * test_predictions_lgbm)\n",
    "# Weighted average of predictions from all three models\n",
    "threshold = 0.5  # Set the threshold for converting probabilities to binary predictions\n",
    "binary_predictions = (ensemble_predictions >= threshold).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Get the 'id' values from the test_data DataFrame\n",
    "submission_ids = test_data['id']\n",
    "\n",
    "# Create binary predictions based on a threshold (e.g., 0.5)\n",
    "threshold = 0.5\n",
    "binary_predictions = (ensemble_predictions >= threshold).astype(int)\n",
    "\n",
    "# Create the submission DataFrame with 'id' and binary 'Target' values\n",
    "submission_df = pd.DataFrame({'id': submission_ids, 'Target': binary_predictions})\n",
    "\n",
    "# Save the submission file to CSV\n",
    "submission_df.to_csv('submission_binary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (78161, 1), indices imply (78161, 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m all_columns \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(numeric_cols) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(encoded_cols)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Convert the preprocessed data array to a DataFrame with proper column names\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m preprocessed_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(X_all_preprocessed, columns\u001b[39m=\u001b[39;49mall_columns)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Save the preprocessed and feature-engineered data to a CSV file\u001b[39;00m\n\u001b[1;32m     11\u001b[0m preprocessed_data\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mpreprocessed_data.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:798\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    790\u001b[0m         mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    791\u001b[0m             arrays,\n\u001b[1;32m    792\u001b[0m             columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    795\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    796\u001b[0m         )\n\u001b[1;32m    797\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 798\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[1;32m    799\u001b[0m             data,\n\u001b[1;32m    800\u001b[0m             index,\n\u001b[1;32m    801\u001b[0m             columns,\n\u001b[1;32m    802\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    803\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    804\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[1;32m    805\u001b[0m         )\n\u001b[1;32m    806\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    808\u001b[0m         {},\n\u001b[1;32m    809\u001b[0m         index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    813\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:337\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    333\u001b[0m index, columns \u001b[39m=\u001b[39m _get_axes(\n\u001b[1;32m    334\u001b[0m     values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns\n\u001b[1;32m    335\u001b[0m )\n\u001b[0;32m--> 337\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    340\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:408\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    406\u001b[0m passed \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[1;32m    407\u001b[0m implied \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(index), \u001b[39mlen\u001b[39m(columns))\n\u001b[0;32m--> 408\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of passed values is \u001b[39m\u001b[39m{\u001b[39;00mpassed\u001b[39m}\u001b[39;00m\u001b[39m, indices imply \u001b[39m\u001b[39m{\u001b[39;00mimplied\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (78161, 1), indices imply (78161, 71)"
     ]
    }
   ],
   "source": [
    "# Define the column names for the preprocessed data\n",
    "numeric_cols = all_data.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = all_data.select_dtypes(include=[object]).columns\n",
    "encoded_cols = preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(categorical_cols)\n",
    "all_columns = list(numeric_cols) + list(encoded_cols)\n",
    "\n",
    "# Convert the preprocessed data array to a DataFrame with proper column names\n",
    "preprocessed_data = pd.DataFrame(X_all_preprocessed, columns=all_columns)\n",
    "\n",
    "# Save the preprocessed and feature-engineered data to a CSV file\n",
    "preprocessed_data.to_csv('preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102127.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Predictions.csv\")\n",
    "df['balance'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_all_preprocessed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# ... (Feature Engineering, Preprocessing, and Transformation)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[39m# Handle class imbalance using BorderlineSMOTE\u001b[39;00m\n\u001b[1;32m     29\u001b[0m smote \u001b[39m=\u001b[39m BorderlineSMOTE(sampling_strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m X_train_resampled, y_train_resampled \u001b[39m=\u001b[39m smote\u001b[39m.\u001b[39mfit_resample(X_all_preprocessed[:train_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]], y_train)\n\u001b[1;32m     32\u001b[0m \u001b[39m# ... (Model Initialization, Training, and Prediction)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[39m# Calculate binary predictions based on the ensemble_predictions and threshold\u001b[39;00m\n\u001b[1;32m     35\u001b[0m binary_predictions \u001b[39m=\u001b[39m (ensemble_predictions \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m threshold)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_all_preprocessed' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "import os\n",
    "\n",
    "# Load the training and test data\n",
    "train_data = pd.read_csv('Train-set.csv')\n",
    "test_data = pd.read_csv('Test-set.csv')\n",
    "\n",
    "# Separate the 'Target' column from the train data\n",
    "y_train = train_data['Target']\n",
    "train_data.drop('Target', axis=1, inplace=True)\n",
    "\n",
    "# Combine train and test data for preprocessing\n",
    "all_data = pd.concat([train_data, test_data], axis=0)\n",
    "\n",
    "# ... (Feature Engineering, Preprocessing, and Transformation)\n",
    "\n",
    "# Handle class imbalance using BorderlineSMOTE\n",
    "smote = BorderlineSMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_all_preprocessed[:train_data.shape[0]], y_train)\n",
    "\n",
    "# ... (Model Initialization, Training, and Prediction)\n",
    "\n",
    "# Calculate binary predictions based on the ensemble_predictions and threshold\n",
    "binary_predictions = (ensemble_predictions >= threshold).astype(int)\n",
    "\n",
    "# Load the true labels for the test set\n",
    "true_labels = pd.read_csv('True-labels.csv')  # Assuming you have a file named 'True-labels.csv' with true labels\n",
    "\n",
    "# Calculate and print classification metrics\n",
    "accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "precision = precision_score(true_labels, binary_predictions)\n",
    "recall = recall_score(true_labels, binary_predictions)\n",
    "f1 = f1_score(true_labels, binary_predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Get the 'id' values from the test_data DataFrame\n",
    "submission_ids = test_data['id']\n",
    "\n",
    "# Create the submission DataFrame with 'id' and binary 'Target' values\n",
    "submission_df = pd.DataFrame({'id': submission_ids, 'Target': binary_predictions})\n",
    "\n",
    "# Save the submission file to CSV\n",
    "submission_df.to_csv('submission_binary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8p/b3ws2sh942n4mms732sp99fr0000gn/T/ipykernel_87016/75874235.py:29: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  all_data['day'] = pd.to_datetime(all_data['day'])\n",
      "/var/folders/8p/b3ws2sh942n4mms732sp99fr0000gn/T/ipykernel_87016/75874235.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  all_data['day'] = pd.to_datetime(all_data['day'], errors='coerce')\n",
      "/Users/macbookpro/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['day_of_week']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 48433, number of negative: 48433\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16946\n",
      "[LightGBM] [Info] Number of data points in the train set: 96866, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'true_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 93\u001b[0m\n\u001b[1;32m     87\u001b[0m binary_predictions \u001b[39m=\u001b[39m (ensemble_predictions \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m threshold)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[39m# Load the true labels for the test set\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m# true_labels = pd.read_csv('True-labels.csv')  # Assuming you have a file named 'True-labels.csv' with true labels\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \n\u001b[1;32m     92\u001b[0m \u001b[39m# Calculate and print classification metrics\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m accuracy \u001b[39m=\u001b[39m accuracy_score(true_labels, binary_predictions)\n\u001b[1;32m     94\u001b[0m precision \u001b[39m=\u001b[39m precision_score(true_labels, binary_predictions)\n\u001b[1;32m     95\u001b[0m recall \u001b[39m=\u001b[39m recall_score(true_labels, binary_predictions)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'true_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "import os\n",
    "\n",
    "# Load the training and test data\n",
    "train_data = pd.read_csv('Train-set.csv')\n",
    "test_data = pd.read_csv('Test-set.csv')\n",
    "\n",
    "# Separate the 'Target' column from the train data\n",
    "y_train = train_data['Target']\n",
    "train_data.drop('Target', axis=1, inplace=True)\n",
    "\n",
    "# Combine train and test data for preprocessing\n",
    "all_data = pd.concat([train_data, test_data], axis=0)\n",
    "\n",
    "# Feature Engineering: Extract day of the week and create a weekend indicator\n",
    "try:\n",
    "    # Convert the 'day' column to a standard date format\n",
    "    all_data['day'] = pd.to_datetime(all_data['day'])\n",
    "    # Calculate the day of the week (0-6) from the date and create a new 'day_of_week' feature\n",
    "    all_data['day_of_week'] = all_data['day'].dt.dayofweek\n",
    "    # Create a binary feature 'is_weekend' to indicate if it's a weekend (Saturday or Sunday)\n",
    "    all_data['is_weekend'] = all_data['day_of_week'].isin([5, 6]).astype(int)\n",
    "    # Remove the original 'day' column as we now have 'day_of_week' and 'is_weekend' features\n",
    "    all_data.drop('day', axis=1, inplace=True)\n",
    "except (ValueError, OverflowError, pd._libs.tslibs.np_datetime.OutOfBoundsDatetime):\n",
    "    # Handle errors due to invalid date formats\n",
    "    all_data['day'] = pd.to_datetime(all_data['day'], errors='coerce')\n",
    "    all_data['day_of_week'] = all_data['day'].dt.dayofweek\n",
    "    all_data['is_weekend'] = all_data['day_of_week'].isin([5, 6]).astype(int)\n",
    "    all_data.drop('day', axis=1, inplace=True)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = all_data.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = all_data.select_dtypes(include=[object]).columns\n",
    "\n",
    "# Create transformers for numeric and categorical columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocess the data using the column transformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "X_all_preprocessed = preprocessor.fit_transform(all_data)\n",
    "\n",
    "# Handle class imbalance using BorderlineSMOTE\n",
    "smote = BorderlineSMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_all_preprocessed[:train_data.shape[0]], y_train)\n",
    "\n",
    "# Create and train optimized models\n",
    "optimized_rf_model = RandomForestClassifier(n_estimators=150, max_depth=9, random_state=42)\n",
    "optimized_gb_model = GradientBoostingClassifier(n_estimators=160, learning_rate=0.05, max_depth=7, random_state=42)\n",
    "optimized_lgbm_model = LGBMClassifier(n_estimators=180, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\n",
    "optimized_rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "optimized_gb_model.fit(X_train_resampled, y_train_resampled)\n",
    "optimized_lgbm_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get predictions using optimized models\n",
    "test_predictions_rf = optimized_rf_model.predict_proba(X_all_preprocessed[train_data.shape[0]:])[:, 1]\n",
    "test_predictions_gb = optimized_gb_model.predict_proba(X_all_preprocessed[train_data.shape[0]:])[:, 1]\n",
    "test_predictions_lgbm = optimized_lgbm_model.predict_proba(X_all_preprocessed[train_data.shape[0]:])[:, 1]\n",
    "\n",
    "# Combine the predictions using weighted averaging\n",
    "ensemble_predictions = (0.4 * test_predictions_rf) + (0.4 * test_predictions_gb) + (0.2 * test_predictions_lgbm)\n",
    "threshold = 0.5\n",
    "binary_predictions = (ensemble_predictions >= threshold).astype(int)\n",
    "\n",
    "# Load the true labels for the test set\n",
    "# true_labels = pd.read_csv('True-labels.csv')  # Assuming you have a file named 'True-labels.csv' with true labels\n",
    "\n",
    "# Calculate and print classification metrics\n",
    "accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "precision = precision_score(true_labels, binary_predictions)\n",
    "recall = recall_score(true_labels, binary_predictions)\n",
    "# f1 = f1_score(true_labels, binary_predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "# print(\"F1-score:\", f1)\n",
    "\n",
    "# Get the 'id' values from the test_data DataFrame\n",
    "submission_ids = test_data['id']\n",
    "\n",
    "# Create binary predictions based on a threshold (e.g., 0.5)\n",
    "threshold = 0.5\n",
    "binary_predictions = (ensemble_predictions >= threshold).astype(int)\n",
    "\n",
    "# Create the submission DataFrame with 'id' and binary 'Target' values\n",
    "submission_df = pd.DataFrame({'id': submission_ids, 'Target': binary_predictions})\n",
    "\n",
    "# Save the submission file to CSV\n",
    "submission_df.to_csv('submission_binary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 'id' values from the test_data DataFrame\n",
    "submission_ids = test_data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Calculate and print classification metrics\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m accuracy \u001b[39m=\u001b[39m accuracy_score(true_labels, binary_predictions)\n\u001b[1;32m      3\u001b[0m precision \u001b[39m=\u001b[39m precision_score(true_labels, binary_predictions)\n\u001b[1;32m      4\u001b[0m recall \u001b[39m=\u001b[39m recall_score(true_labels, binary_predictions)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'true_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate and print classification metrics\n",
    "accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "precision = precision_score(true_labels, binary_predictions)\n",
    "recall = recall_score(true_labels, binary_predictions)\n",
    "# f1 = f1_score(true_labels, binary_predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "# print(\"F1-score:\", f1)\n",
    "\n",
    "# Get the 'id' values from the test_data DataFrame\n",
    "submission_ids = test_data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: -0.9963351129495861\n",
      "Mean Squared Error: 0.4990831165508124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ... (your existing code for preprocessing, training, and prediction)\n",
    "\n",
    "# Calculate binary predictions based on the ensemble_predictions and threshold\n",
    "binary_predictions = (ensemble_predictions >= threshold).astype(int)\n",
    "\n",
    "# Create the submission DataFrame with 'id' and binary 'Target' values\n",
    "submission_df = pd.DataFrame({'id': submission_ids, 'Target': binary_predictions})\n",
    "\n",
    "# Save the submission file to CSV\n",
    "submission_df.to_csv('submission_binary.csv', index=False)\n",
    "\n",
    "# Calculate R^2 and mean squared error (MSE)\n",
    "# Note: These metrics are not directly suitable for classification, so use them with caution\n",
    "# Replace hypothetical_true_labels with the appropriate values if you have them\n",
    "hypothetical_true_labels = np.random.randint(2, size=len(binary_predictions))\n",
    "r2 = r2_score(hypothetical_true_labels, binary_predictions)\n",
    "mse = mean_squared_error(hypothetical_true_labels, binary_predictions)\n",
    "\n",
    "print(\"R^2:\", r2)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1073576489.py, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 35\u001b[0;36m\u001b[0m\n\u001b[0;31m    numeric_cols = all_data.select_dtypes(include=[np.number]).columns\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "import os\n",
    "\n",
    "# Load the training and test data\n",
    "train_data = pd.read_csv('Train-set.csv')\n",
    "test_data = pd.read_csv('Test-set.csv')\n",
    "\n",
    "# Separate the 'Target' column from the train data\n",
    "y_train = train_data['Target']\n",
    "train_data.drop('Target', axis=1, inplace=True)\n",
    "\n",
    "# Combine train and test data for preprocessing\n",
    "all_data = pd.concat([train_data, test_data], axis=0)\n",
    "\n",
    "# Feature Engineering: Extract day of the week and create a weekend indicator\n",
    "try:\n",
    "    # Convert 'day' column to datetime\n",
    "    all_data['day'] = pd.to_datetime(all_data['day'])\n",
    "    # ... (rest of your feature engineering code)\n",
    "except (ValueError, OverflowError, pd._libs.tslibs.np_datetime.OutOfBoundsDatetime):\n",
    "    # ... (handling errors)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = all_data.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = all_data.select_dtypes(include=[object]).columns\n",
    "\n",
    "# ... (rest of your preprocessing code)\n",
    "\n",
    "# ... (rest of your resampling and model training code)\n",
    "\n",
    "# Combine the predictions using weighted averaging\n",
    "ensemble_predictions = (0.4 * test_predictions_rf) + (0.4 * test_predictions_gb) + (0.2 * test_predictions_lgbm)\n",
    "\n",
    "# Calculate R-squared for the regression of predicted probabilities\n",
    "mean_actual = y_train_resampled.mean()\n",
    "ss_total = ((y_train_resampled - mean_actual) ** 2).sum()\n",
    "ss_residual = ((y_train_resampled - ensemble_predictions) ** 2).sum()\n",
    "r_squared = 1 - (ss_residual / ss_total)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for the regression of predicted probabilities\n",
    "mse = ss_residual / len(y_train_resampled)\n",
    "\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Convert probabilities to binary predictions based on the threshold\n",
    "binary_predictions = (ensemble_predictions >= threshold).astype(int)\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(y_train_resampled, binary_predictions)\n",
    "precision = precision_score(y_train_resampled, binary_predictions)\n",
    "recall = recall_score(y_train_resampled, binary_predictions)\n",
    "f1 = f1_score(y_train_resampled, binary_predictions)\n",
    "roc_auc = roc_auc_score(y_train_resampled, ensemble_predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"ROC-AUC:\", roc_auc)\n",
    "\n",
    "# Create the submission DataFrame with 'id' and binary 'Target' values\n",
    "submission_df = pd.DataFrame({'id': submission_ids, 'Target': binary_predictions})\n",
    "\n",
    "# Save the submission file to CSV\n",
    "submission_df.to_csv('submission_binary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2203037766.py, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 35\u001b[0;36m\u001b[0m\n\u001b[0;31m    numeric_cols = all_data.select_dtypes(include=[np.number]).columns\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "import os\n",
    "\n",
    "# Load the training and test data\n",
    "train_data = pd.read_csv('Train-set.csv')\n",
    "test_data = pd.read_csv('Test-set.csv')\n",
    "\n",
    "# Separate the 'Target' column from the train data\n",
    "y_train = train_data['Target']\n",
    "train_data.drop('Target', axis=1, inplace=True)\n",
    "\n",
    "# Combine train and test data for preprocessing\n",
    "all_data = pd.concat([train_data, test_data], axis=0)\n",
    "\n",
    "# Feature Engineering: Extract day of the week and create a weekend indicator\n",
    "try:\n",
    "    # Convert 'day' column to datetime\n",
    "    all_data['day'] = pd.to_datetime(all_data['day'])\n",
    "    # ... (rest of your feature engineering code)\n",
    "except (ValueError, OverflowError, pd._libs.tslibs.np_datetime.OutOfBoundsDatetime):\n",
    "    # ... (handling errors)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = all_data.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = all_data.select_dtypes(include=[object]).columns\n",
    "\n",
    "# Create transformers for numeric and categorical columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocess the data using the column transformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "X_all_preprocessed = preprocessor.fit_transform(all_data)\n",
    "\n",
    "# Handle class imbalance using BorderlineSMOTE\n",
    "smote = BorderlineSMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_all_preprocessed[:train_data.shape[0]], y_train)\n",
    "\n",
    "# Create and train optimized models\n",
    "optimized_rf_model = RandomForestClassifier(n_estimators=150, max_depth=9, random_state=42)\n",
    "optimized_gb_model = GradientBoostingClassifier(n_estimators=160, learning_rate=0.05, max_depth=7, random_state=42)\n",
    "optimized_lgbm_model = LGBMClassifier(n_estimators=180, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\n",
    "optimized_rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "optimized_gb_model.fit(X_train_resampled, y_train_resampled)\n",
    "optimized_lgbm_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get predictions using optimized models\n",
    "test_predictions_rf = optimized_rf_model.predict_proba(X_all_preprocessed[train_data.shape[0]:])[:, 1]\n",
    "test_predictions_gb = optimized_gb_model.predict_proba(X_all_preprocessed[train_data.shape[0]:])[:, 1]\n",
    "test_predictions_lgbm = optimized_lgbm_model.predict_proba(X_all_preprocessed[train_data.shape[0]:])[:, 1]\n",
    "\n",
    "# Combine the predictions using weighted averaging\n",
    "ensemble_predictions = (0.4 * test_predictions_rf) + (0.4 * test_predictions_gb) + (0.2 * test_predictions_lgbm)\n",
    "threshold = 0.5\n",
    "binary_predictions = (ensemble_predictions >= threshold).astype(int)\n",
    "\n",
    "# Get the 'id' values from the test_data DataFrame\n",
    "submission_ids = test_data['id']\n",
    "\n",
    "# Create the submission DataFrame with 'id' and binary 'Target' values\n",
    "submission_df = pd.DataFrame({'id': submission_ids, 'Target': binary_predictions})\n",
    "\n",
    "# Calculate R-squared for the regression of predicted probabilities\n",
    "mean_actual = y_train_resampled.mean()\n",
    "ss_total = ((y_train_resampled - mean_actual) ** 2).sum()\n",
    "ss_residual = ((y_train_resampled - ensemble_predictions) ** 2).sum()\n",
    "r_squared = 1 - (ss_residual / ss_total)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for the regression of predicted probabilities\n",
    "mse = ss_residual / len(y_train_resampled)\n",
    "\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(y_train_resampled, binary_predictions)\n",
    "precision = precision_score(y_train_resampled, binary_predictions)\n",
    "recall = recall_score(y_train_resampled, binary_predictions)\n",
    "f1 = f1_score(y_train_resampled, binary_predictions)\n",
    "roc_auc = roc_auc_score(y_train_resampled, ensemble_predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"ROC-AUC:\", roc_auc)\n",
    "\n",
    "# Save the submission file to CSV\n",
    "submission_df.to_csv('submission_binary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1511361963.py, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 35\u001b[0;36m\u001b[0m\n\u001b[0;31m    numeric_cols = all_data.select_dtypes(include=[np.number]).columns\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "import os\n",
    "\n",
    "# Load the training and test data\n",
    "train_data = pd.read_csv('Train-set.csv')\n",
    "test_data = pd.read_csv('Test-set.csv')\n",
    "\n",
    "# Separate the 'Target' column from the train data\n",
    "y_train = train_data['Target']\n",
    "train_data.drop('Target', axis=1, inplace=True)\n",
    "\n",
    "# Combine train and test data for preprocessing\n",
    "all_data = pd.concat([train_data, test_data], axis=0)\n",
    "\n",
    "# Feature Engineering: Extract day of the week and create a weekend indicator\n",
    "try:\n",
    "    # Convert 'day' column to datetime\n",
    "    all_data['day'] = pd.to_datetime(all_data['day'])\n",
    "    # ... (rest of your feature engineering code)\n",
    "except (ValueError, OverflowError, pd._libs.tslibs.np_datetime.OutOfBoundsDatetime):\n",
    "    # ... (handling errors)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = all_data.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = all_data.select_dtypes(include=[object]).columns\n",
    "\n",
    "# Create transformers for numeric and categorical columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocess the data using the column transformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "X_all_preprocessed = preprocessor.fit_transform(all_data)\n",
    "\n",
    "# Handle class imbalance using BorderlineSMOTE\n",
    "smote = BorderlineSMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_all_preprocessed[:train_data.shape[0]], y_train)\n",
    "\n",
    "# Create and train optimized models\n",
    "optimized_rf_model = RandomForestClassifier(n_estimators=150, max_depth=9, random_state=42)\n",
    "optimized_gb_model = GradientBoostingClassifier(n_estimators=160, learning_rate=0.05, max_depth=7, random_state=42)\n",
    "optimized_lgbm_model = LGBMClassifier(n_estimators=180, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\n",
    "optimized_rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "optimized_gb_model.fit(X_train_resampled, y_train_resampled)\n",
    "optimized_lgbm_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get predictions using optimized models\n",
    "test_predictions_rf = optimized_rf_model.predict_proba(X_all_preprocessed[train_data.shape[0]:])[:, 1]\n",
    "test_predictions_gb = optimized_gb_model.predict_proba(X_all_preprocessed[train_data.shape[0]:])[:, 1]\n",
    "test_predictions_lgbm = optimized_lgbm_model.predict_proba(X_all_preprocessed[train_data.shape[0]:])[:, 1]\n",
    "\n",
    "# Combine the predictions using weighted averaging\n",
    "ensemble_predictions = (0.4 * test_predictions_rf) + (0.4 * test_predictions_gb) + (0.2 * test_predictions_lgbm)\n",
    "threshold = 0.5\n",
    "binary_predictions = (ensemble_predictions >= threshold).astype(int)\n",
    "\n",
    "# Get the 'id' values from the test_data DataFrame\n",
    "submission_ids = test_data['id']\n",
    "\n",
    "# Create the submission DataFrame with 'id' and binary 'Target' values\n",
    "submission_df = pd.DataFrame({'id': submission_ids, 'Target': binary_predictions})\n",
    "\n",
    "# Save the submission file to CSV\n",
    "submission_df.to_csv('submission_binary.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
